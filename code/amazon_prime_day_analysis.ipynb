{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "import re, os, sys, string, gensim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from itertools import chain\n",
    "from collections import Counter\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "\n",
    "# preprocessing library for tweet data\n",
    "# https://pypi.org/project/tweet-preprocessor/\n",
    "import preprocessor as p\n",
    "from gensim import corpora\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "lemma = WordNetLemmatizer()\n",
    "from nltk import word_tokenize, ngrams\n",
    "\n",
    "pd.set_option('display.max_columns', 20)\n",
    "pd.set_option('display.max_rows', 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading File which contains Tweet for Amazon Prime Sales Day "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skipping line 6485: expected 10 fields, saw 11\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_file_path = '../data/output_#primeday_.csv'\n",
    "data_tweet = pd.read_csv(input_file_path,error_bad_lines=False,sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>username</th>\n",
       "      <th>date</th>\n",
       "      <th>retweets</th>\n",
       "      <th>favorites</th>\n",
       "      <th>text</th>\n",
       "      <th>geo</th>\n",
       "      <th>mentions</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>id</th>\n",
       "      <th>permalink</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>frlorente</td>\n",
       "      <td>2018-07-18 11:37</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Imagen del éxito del #PrimeDay de @AmazonESP :...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@AmazonESP @AmazonEnLucha</td>\n",
       "      <td>#PrimeDay #HuelgaAmazon</td>\n",
       "      <td>1019463538052657152</td>\n",
       "      <td>https://twitter.com/frlorente/status/101946353...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Beyonsike</td>\n",
       "      <td>2018-07-18 11:36</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>uhh idk how this twitter shit work #LOVEis #Al...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#LOVEis #AllStarGame #ASG2018 #LevelUp #TrumpA...</td>\n",
       "      <td>1019463505555197952</td>\n",
       "      <td>https://twitter.com/Beyonsike/status/101946350...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>startuprad_io</td>\n",
       "      <td>2018-07-18 11:36</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Employees of #Amazon in Leipzig go in #strike ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#Amazon #strike #PrimeDay</td>\n",
       "      <td>1019463404266905600</td>\n",
       "      <td>https://twitter.com/startuprad_io/status/10194...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        username              date  retweets  favorites  \\\n",
       "0      frlorente  2018-07-18 11:37         0          0   \n",
       "1      Beyonsike  2018-07-18 11:36         0          0   \n",
       "2  startuprad_io  2018-07-18 11:36         1          1   \n",
       "\n",
       "                                                text  geo  \\\n",
       "0  Imagen del éxito del #PrimeDay de @AmazonESP :...  NaN   \n",
       "1  uhh idk how this twitter shit work #LOVEis #Al...  NaN   \n",
       "2  Employees of #Amazon in Leipzig go in #strike ...  NaN   \n",
       "\n",
       "                    mentions  \\\n",
       "0  @AmazonESP @AmazonEnLucha   \n",
       "1                        NaN   \n",
       "2                        NaN   \n",
       "\n",
       "                                            hashtags                   id  \\\n",
       "0                            #PrimeDay #HuelgaAmazon  1019463538052657152   \n",
       "1  #LOVEis #AllStarGame #ASG2018 #LevelUp #TrumpA...  1019463505555197952   \n",
       "2                          #Amazon #strike #PrimeDay  1019463404266905600   \n",
       "\n",
       "                                           permalink  \n",
       "0  https://twitter.com/frlorente/status/101946353...  \n",
       "1  https://twitter.com/Beyonsike/status/101946350...  \n",
       "2  https://twitter.com/startuprad_io/status/10194...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_tweet.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In above dataframe, \"text\" column contains tweet \n",
    "data_tweet.drop_duplicates('text',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of Tweets are : 6320\n"
     ]
    }
   ],
   "source": [
    "print(\"No of Tweets are : {}\".format(data_tweet.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing tweets for finding sentiment - Cleaning of Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stop = set(stopwords.words('english'))\n",
    "add_stop=['http','u','s']\n",
    "stop = stop.union(add_stop)\n",
    "# exclude = set(string.punctuation)\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "def cleaning_tweets(tweet):\n",
    "    # Setting Preprocessor paramters - parts to clean from tweet \n",
    "    p.set_options(p.OPT.URL, p.OPT.NUMBER)\n",
    "    sentence = str(tweet)\n",
    "    sentence = p.clean(sentence)\n",
    "    reg_http=r\"(http)(s)?(://)?\"\n",
    "    sentence = re.sub(reg_http,\"\", sentence)\n",
    "    stop_free = \" \".join([i for i in sentence.lower().split() if i not in stop])\n",
    "    #sentence = re.sub(\"[^A-Za-z .#]+\",\"\", stop_free)\n",
    "    return stop_free"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_tweet['tweet_preprocessed'] = data_tweet['text'].apply(cleaning_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_preprocessed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Imagen del éxito del #PrimeDay de @AmazonESP :...</td>\n",
       "      <td>imagen del éxito del #primeday de @amazonesp :...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>uhh idk how this twitter shit work #LOVEis #Al...</td>\n",
       "      <td>uhh idk twitter shit work #loveis #allstargame...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Employees of #Amazon in Leipzig go in #strike ...</td>\n",
       "      <td>employees #amazon leipzig go #strike #primeday...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Conjunto de utensilios de cocina baja un 17% h...</td>\n",
       "      <td>conjunto de utensilios de cocina baja un % 21 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>So far 100% of my packages from #primeday are ...</td>\n",
       "      <td>far % packages #primeday shipped $ amzn logist...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Na Black Friday en Singles Day hebben we nu bl...</td>\n",
       "      <td>na black friday en singles day hebben nu blijk...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>As Bezos Becomes Richest Man in Modern History...</td>\n",
       "      <td>bezos becomes richest man modern history, amaz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>#PrimeDay is bad for 1 thing though: taking ad...</td>\n",
       "      <td>#primeday bad thing though: taking advantage a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Almost bought a ukulele on $ amzn bcuz #PrimeDay</td>\n",
       "      <td>almost bought ukulele $ amzn bcuz #primeday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Castle 240RGB ou Castle 280RGB, quelle taille ...</td>\n",
       "      <td>castle rgb ou castle rgb, quelle taille est ce...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  Imagen del éxito del #PrimeDay de @AmazonESP :...   \n",
       "1  uhh idk how this twitter shit work #LOVEis #Al...   \n",
       "2  Employees of #Amazon in Leipzig go in #strike ...   \n",
       "3  Conjunto de utensilios de cocina baja un 17% h...   \n",
       "4  So far 100% of my packages from #primeday are ...   \n",
       "5  Na Black Friday en Singles Day hebben we nu bl...   \n",
       "6  As Bezos Becomes Richest Man in Modern History...   \n",
       "7  #PrimeDay is bad for 1 thing though: taking ad...   \n",
       "8   Almost bought a ukulele on $ amzn bcuz #PrimeDay   \n",
       "9  Castle 240RGB ou Castle 280RGB, quelle taille ...   \n",
       "\n",
       "                                  tweet_preprocessed  \n",
       "0  imagen del éxito del #primeday de @amazonesp :...  \n",
       "1  uhh idk twitter shit work #loveis #allstargame...  \n",
       "2  employees #amazon leipzig go #strike #primeday...  \n",
       "3  conjunto de utensilios de cocina baja un % 21 ...  \n",
       "4  far % packages #primeday shipped $ amzn logist...  \n",
       "5  na black friday en singles day hebben nu blijk...  \n",
       "6  bezos becomes richest man modern history, amaz...  \n",
       "7  #primeday bad thing though: taking advantage a...  \n",
       "8        almost bought ukulele $ amzn bcuz #primeday  \n",
       "9  castle rgb ou castle rgb, quelle taille est ce...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_tweet[['text','tweet_preprocessed']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Anlaysis using Vader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "analyzer = SentimentIntensityAnalyzer()\n",
    "def find_polarity_score(tweet):\n",
    "    score_cleaned = analyzer.polarity_scores(str(tweet))\n",
    "    return score_cleaned['pos'],score_cleaned['neg'],score_cleaned['neu'],score_cleaned['compound']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_tweet['positive_score'],data_tweet['negative_score'],data_tweet['neutral_score'],data_tweet['compound_score']=\\\n",
    "zip(*data_tweet['tweet_preprocessed'].apply(find_polarity_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>username</th>\n",
       "      <th>date</th>\n",
       "      <th>retweets</th>\n",
       "      <th>favorites</th>\n",
       "      <th>text</th>\n",
       "      <th>geo</th>\n",
       "      <th>mentions</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>id</th>\n",
       "      <th>permalink</th>\n",
       "      <th>tweet_preprocessed</th>\n",
       "      <th>positive_score</th>\n",
       "      <th>negative_score</th>\n",
       "      <th>neutral_score</th>\n",
       "      <th>compound_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>frlorente</td>\n",
       "      <td>2018-07-18 11:37</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Imagen del éxito del #PrimeDay de @AmazonESP :...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@AmazonESP @AmazonEnLucha</td>\n",
       "      <td>#PrimeDay #HuelgaAmazon</td>\n",
       "      <td>1019463538052657152</td>\n",
       "      <td>https://twitter.com/frlorente/status/101946353...</td>\n",
       "      <td>imagen del éxito del #primeday de @amazonesp :...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Beyonsike</td>\n",
       "      <td>2018-07-18 11:36</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>uhh idk how this twitter shit work #LOVEis #Al...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#LOVEis #AllStarGame #ASG2018 #LevelUp #TrumpA...</td>\n",
       "      <td>1019463505555197952</td>\n",
       "      <td>https://twitter.com/Beyonsike/status/101946350...</td>\n",
       "      <td>uhh idk twitter shit work #loveis #allstargame...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.263</td>\n",
       "      <td>0.737</td>\n",
       "      <td>-0.6124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>startuprad_io</td>\n",
       "      <td>2018-07-18 11:36</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Employees of #Amazon in Leipzig go in #strike ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#Amazon #strike #PrimeDay</td>\n",
       "      <td>1019463404266905600</td>\n",
       "      <td>https://twitter.com/startuprad_io/status/10194...</td>\n",
       "      <td>employees #amazon leipzig go #strike #primeday...</td>\n",
       "      <td>0.146</td>\n",
       "      <td>0.151</td>\n",
       "      <td>0.704</td>\n",
       "      <td>0.2263</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        username              date  retweets  favorites  \\\n",
       "0      frlorente  2018-07-18 11:37         0          0   \n",
       "1      Beyonsike  2018-07-18 11:36         0          0   \n",
       "2  startuprad_io  2018-07-18 11:36         1          1   \n",
       "\n",
       "                                                text  geo  \\\n",
       "0  Imagen del éxito del #PrimeDay de @AmazonESP :...  NaN   \n",
       "1  uhh idk how this twitter shit work #LOVEis #Al...  NaN   \n",
       "2  Employees of #Amazon in Leipzig go in #strike ...  NaN   \n",
       "\n",
       "                    mentions  \\\n",
       "0  @AmazonESP @AmazonEnLucha   \n",
       "1                        NaN   \n",
       "2                        NaN   \n",
       "\n",
       "                                            hashtags                   id  \\\n",
       "0                            #PrimeDay #HuelgaAmazon  1019463538052657152   \n",
       "1  #LOVEis #AllStarGame #ASG2018 #LevelUp #TrumpA...  1019463505555197952   \n",
       "2                          #Amazon #strike #PrimeDay  1019463404266905600   \n",
       "\n",
       "                                           permalink  \\\n",
       "0  https://twitter.com/frlorente/status/101946353...   \n",
       "1  https://twitter.com/Beyonsike/status/101946350...   \n",
       "2  https://twitter.com/startuprad_io/status/10194...   \n",
       "\n",
       "                                  tweet_preprocessed  positive_score  \\\n",
       "0  imagen del éxito del #primeday de @amazonesp :...           0.000   \n",
       "1  uhh idk twitter shit work #loveis #allstargame...           0.000   \n",
       "2  employees #amazon leipzig go #strike #primeday...           0.146   \n",
       "\n",
       "   negative_score  neutral_score  compound_score  \n",
       "0           0.000          1.000          0.0000  \n",
       "1           0.263          0.737         -0.6124  \n",
       "2           0.151          0.704          0.2263  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_tweet.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making class of Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "neg_beg = -1\n",
    "neg_end = -0.4\n",
    "neu_end = 0.4\n",
    "pos_end = 1\n",
    "scale=[neg_beg,neg_end,neu_end,pos_end]\n",
    "labels=['negative','neutral','positive']\n",
    "data_tweet['sentiment']= pd.cut(data_tweet['compound_score'], bins = scale,labels=labels,include_lowest=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "neutral     3503\n",
       "positive    2407\n",
       "negative     410\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_tweet['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making different files for each sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative\n",
      "neutral\n",
      "positive\n"
     ]
    }
   ],
   "source": [
    "for label in labels:\n",
    "    print(label)\n",
    "    classified_df = data_tweet[data_tweet['sentiment'] == label]\n",
    "    classified_df.to_csv('../output/'+label+'_tweets.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Percentage distribution of sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "no_pos = data_tweet[data_tweet['sentiment']=='positive'].shape[0]\n",
    "no_neg = data_tweet[data_tweet['sentiment']=='negative'].shape[0]\n",
    "no_neu = data_tweet[data_tweet['sentiment']=='neutral'].shape[0]\n",
    "no_tweets = data_tweet.shape[0]\n",
    "pos_per = (no_pos/no_tweets)*100\n",
    "neg_per = (no_neg/no_tweets)*100\n",
    "neu_per = (no_neu/no_tweets)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total No of tweets is 6320\n",
      "Percentage of Positive Tweets 38.085443038%\n",
      "Percentage of Negative Tweets 6.48734177215%\n",
      "Percentage of Neutral Tweets 55.4272151899%\n"
     ]
    }
   ],
   "source": [
    "print(\"Total No of tweets is {}\".format(no_tweets))\n",
    "print(\"Percentage of Positive Tweets {}%\".format(pos_per))\n",
    "print(\"Percentage of Negative Tweets {}%\".format(neg_per))\n",
    "print(\"Percentage of Neutral Tweets {}%\".format(neu_per))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "height = [pos_per,neu_per,neg_per]\n",
    "bars = ['Positive','Neutral','Negative']\n",
    "y_pos = np.arange(len(bars))\n",
    "bar_width = 0.30\n",
    "plt.figure(figsize=(8, 7))\n",
    "plt.bar(y_pos, height,bar_width,color=['blue','orange','red'],alpha=0.5,align='center')\n",
    "\n",
    "plt.xticks(y_pos, bars)\n",
    "plt.suptitle('Twitter Sentiment : Amazon Prime Day', fontsize=18,fontweight=\"bold\")\n",
    "plt.ylabel('Percentage of Tweets (%)',fontsize=12)\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "plt.savefig('../output/sentiment_stat.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA - Topic Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing for Topic Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rem_manual_words(tweet):\n",
    "    com_words = ['#amazon','amazon','#primeday','#','.','primeday','@amazon','@primeday','prime','day','%',\\\n",
    "                 '#primeday2018','get','take','ends','end','shop','shopping','\\x80\\xe2','\\u2026',\\\n",
    "                '\\xe2\\x80\\xa6','#amazonprimeday','still','hour','hours','sale','today','make','even',\n",
    "                'use','today','deal','save','purchase','got','far','buy'] \n",
    "    return \" \".join(i for i in tweet.split() if i not in com_words)\n",
    "\n",
    "def cleaning(tweet):\n",
    "    sentence = re.sub(\"[^A-Za-z\\ ]+\",\"\", tweet)\n",
    "    return \" \".join(lemma.lemmatize(word) for word in sentence.split())\n",
    "\n",
    "data_tweet['tweet_preprocessed_remove_common_words'] = data_tweet['tweet_preprocessed'].apply(rem_manual_words)\n",
    "data_tweet['tweet_preprocessed_remove_common_words'] = data_tweet['tweet_preprocessed_remove_common_words'].apply(cleaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    imagen del xito del de amazonesp descuentos en...\n",
       "1    uhh idk twitter shit work loveis allstargame a...\n",
       "2    employee leipzig go strike demand better work ...\n",
       "3    conjunto de utensilios de cocina baja un bbsbr...\n",
       "4    package shipped amzn logistics watch uspsupsfe...\n",
       "Name: tweet_preprocessed_remove_common_words, dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_tweet['tweet_preprocessed_remove_common_words'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc_complete = list(data_tweet['tweet_preprocessed_remove_common_words'])\n",
    "doc_clean = [doc.split() for doc in doc_complete]\n",
    "dictionary = corpora.Dictionary(doc_clean)\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]\n",
    "Lda = gensim.models.ldamodel.LdaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(121, 1), (122, 1), (123, 2), (124, 1), (125, 1), (126, 1)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dictionary.token2id['basketball']\n",
    "# dictionary.doc2bow(doc)\n",
    "# doc\n",
    "doc_term_matrix[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class LdaModel in module gensim.models.ldamodel:\n",
      "\n",
      "class LdaModel(gensim.interfaces.TransformationABC, gensim.models.basemodel.BaseTopicModel)\n",
      " |  The constructor estimates Latent Dirichlet Allocation model parameters based\n",
      " |  on a training corpus:\n",
      " |  \n",
      " |  >>> lda = LdaModel(corpus, num_topics=10)\n",
      " |  \n",
      " |  You can then infer topic distributions on new, unseen documents, with\n",
      " |  \n",
      " |  >>> doc_lda = lda[doc_bow]\n",
      " |  \n",
      " |  The model can be updated (trained) with new documents via\n",
      " |  \n",
      " |  >>> lda.update(other_corpus)\n",
      " |  \n",
      " |  Model persistency is achieved through its `load`/`save` methods.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      LdaModel\n",
      " |      gensim.interfaces.TransformationABC\n",
      " |      gensim.utils.SaveLoad\n",
      " |      gensim.models.basemodel.BaseTopicModel\n",
      " |      __builtin__.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __getitem__(self, bow, eps=None)\n",
      " |      Return topic distribution for the given document `bow`, as a list of\n",
      " |      (topic_id, topic_probability) 2-tuples.\n",
      " |      \n",
      " |      Ignore topics with very low probability (below `eps`).\n",
      " |  \n",
      " |  __init__(self, corpus=None, num_topics=100, id2word=None, distributed=False, chunksize=2000, passes=1, update_every=1, alpha='symmetric', eta=None, decay=0.5, offset=1.0, eval_every=10, iterations=50, gamma_threshold=0.001, minimum_probability=0.01, random_state=None, ns_conf={}, minimum_phi_value=0.01, per_word_topics=False)\n",
      " |      If given, start training from the iterable `corpus` straight away. If not given,\n",
      " |      the model is left untrained (presumably because you want to call `update()` manually).\n",
      " |      \n",
      " |      `num_topics` is the number of requested latent topics to be extracted from\n",
      " |      the training corpus.\n",
      " |      \n",
      " |      `id2word` is a mapping from word ids (integers) to words (strings). It is\n",
      " |      used to determine the vocabulary size, as well as for debugging and topic\n",
      " |      printing.\n",
      " |      \n",
      " |      `alpha` and `eta` are hyperparameters that affect sparsity of the document-topic\n",
      " |      (theta) and topic-word (lambda) distributions. Both default to a symmetric\n",
      " |      1.0/num_topics prior.\n",
      " |      \n",
      " |      `alpha` can be set to an explicit array = prior of your choice. It also\n",
      " |      support special values of 'asymmetric' and 'auto': the former uses a fixed\n",
      " |      normalized asymmetric 1.0/topicno prior, the latter learns an asymmetric\n",
      " |      prior directly from your data.\n",
      " |      \n",
      " |      `eta` can be a scalar for a symmetric prior over topic/word\n",
      " |      distributions, or a vector of shape num_words, which can be used to\n",
      " |      impose (user defined) asymmetric priors over the word distribution.\n",
      " |      It also supports the special value 'auto', which learns an asymmetric\n",
      " |      prior over words directly from your data. `eta` can also be a matrix\n",
      " |      of shape num_topics x num_words, which can be used to impose\n",
      " |      asymmetric priors over the word distribution on a per-topic basis\n",
      " |      (can not be learned from data).\n",
      " |      \n",
      " |      Turn on `distributed` to force distributed computing (see the `web tutorial <http://radimrehurek.com/gensim/distributed.html>`_\n",
      " |      on how to set up a cluster of machines for gensim).\n",
      " |      \n",
      " |      Calculate and log perplexity estimate from the latest mini-batch every\n",
      " |      `eval_every` model updates (setting this to 1 slows down training ~2x;\n",
      " |      default is 10 for better performance). Set to None to disable perplexity estimation.\n",
      " |      \n",
      " |      `decay` and `offset` parameters are the same as Kappa and Tau_0 in\n",
      " |      Hoffman et al, respectively.\n",
      " |      \n",
      " |      `minimum_probability` controls filtering the topics returned for a document (bow).\n",
      " |      \n",
      " |      `random_state` can be a np.random.RandomState object or the seed for one\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      >>> lda = LdaModel(corpus, num_topics=100)  # train model\n",
      " |      >>> print(lda[doc_bow]) # get topic probability distribution for a document\n",
      " |      >>> lda.update(corpus2) # update the LDA model with additional documents\n",
      " |      >>> print(lda[doc_bow])\n",
      " |      \n",
      " |      >>> lda = LdaModel(corpus, num_topics=50, alpha='auto', eval_every=5)  # train asymmetric alpha from data\n",
      " |  \n",
      " |  __str__(self)\n",
      " |  \n",
      " |  bound(self, corpus, gamma=None, subsample_ratio=1.0)\n",
      " |      Estimate the variational bound of documents from `corpus`:\n",
      " |      E_q[log p(corpus)] - E_q[log q(corpus)]\n",
      " |      \n",
      " |      `gamma` are the variational parameters on topic weights for each `corpus`\n",
      " |      document (=2d matrix=what comes out of `inference()`).\n",
      " |      If not supplied, will be inferred from the model.\n",
      " |  \n",
      " |  clear(self)\n",
      " |      Clear model state (free up some memory). Used in the distributed algo.\n",
      " |  \n",
      " |  diff(self, other, distance='kullback_leibler', num_words=100, n_ann_terms=10, normed=True)\n",
      " |      Calculate difference topic2topic between two Lda models\n",
      " |      `other` instances of `LdaMulticore` or `LdaModel`\n",
      " |      `distance` is function that will be applied to calculate difference between any topic pair.\n",
      " |      Available values: `kullback_leibler`, `hellinger` and `jaccard`\n",
      " |      `num_words` is quantity of most relevant words that used if distance == `jaccard` (also used for annotation)\n",
      " |      `n_ann_terms` is max quantity of words in intersection/symmetric difference between topics (used for annotation)\n",
      " |      Returns a matrix Z with shape (m1.num_topics, m2.num_topics), where Z[i][j] - difference between topic_i and topic_j\n",
      " |      and matrix annotation with shape (m1.num_topics, m2.num_topics, 2, None),\n",
      " |      where:\n",
      " |      \n",
      " |          annotation[i][j] = [[`int_1`, `int_2`, ...], [`diff_1`, `diff_2`, ...]] and\n",
      " |          `int_k` is word from intersection of `topic_i` and `topic_j` and\n",
      " |          `diff_l` is word from symmetric difference of `topic_i` and `topic_j`\n",
      " |          `normed` is a flag. If `true`, matrix Z will be normalized\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      >>> m1, m2 = LdaMulticore.load(path_1), LdaMulticore.load(path_2)\n",
      " |      >>> mdiff, annotation = m1.diff(m2)\n",
      " |      >>> print(mdiff) # get matrix with difference for each topic pair from `m1` and `m2`\n",
      " |      >>> print(annotation) # get array with positive/negative words for each topic pair from `m1` and `m2`\n",
      " |  \n",
      " |  do_estep(self, chunk, state=None)\n",
      " |      Perform inference on a chunk of documents, and accumulate the collected\n",
      " |      sufficient statistics in `state` (or `self.state` if None).\n",
      " |  \n",
      " |  do_mstep(self, rho, other, extra_pass=False)\n",
      " |      M step: use linear interpolation between the existing topics and\n",
      " |      collected sufficient statistics in `other` to update the topics.\n",
      " |  \n",
      " |  get_document_topics(self, bow, minimum_probability=None, minimum_phi_value=None, per_word_topics=False)\n",
      " |      Return topic distribution for the given document `bow`, as a list of\n",
      " |      (topic_id, topic_probability) 2-tuples.\n",
      " |      \n",
      " |      Ignore topics with very low probability (below `minimum_probability`).\n",
      " |      \n",
      " |      If per_word_topics is True, it also returns a list of topics, sorted in descending order of most likely topics for that word.\n",
      " |      It also returns a list of word_ids and each words corresponding topics' phi_values, multiplied by feature length (i.e, word count)\n",
      " |  \n",
      " |  get_term_topics(self, word_id, minimum_probability=None)\n",
      " |      Returns most likely topics for a particular word in vocab.\n",
      " |  \n",
      " |  get_topic_terms(self, topicid, topn=10)\n",
      " |      Return a list of `(word_id, probability)` 2-tuples for the most\n",
      " |      probable words in topic `topicid`.\n",
      " |      \n",
      " |      Only return 2-tuples for the topn most probable words (ignore the rest).\n",
      " |  \n",
      " |  inference(self, chunk, collect_sstats=False)\n",
      " |      Given a chunk of sparse document vectors, estimate gamma (parameters\n",
      " |      controlling the topic weights) for each document in the chunk.\n",
      " |      \n",
      " |      This function does not modify the model (=is read-only aka const). The\n",
      " |      whole input chunk of document is assumed to fit in RAM; chunking of a\n",
      " |      large corpus must be done earlier in the pipeline.\n",
      " |      \n",
      " |      If `collect_sstats` is True, also collect sufficient statistics needed\n",
      " |      to update the model's topic-word distributions, and return a 2-tuple\n",
      " |      `(gamma, sstats)`. Otherwise, return `(gamma, None)`. `gamma` is of shape\n",
      " |      `len(chunk) x self.num_topics`.\n",
      " |      \n",
      " |      Avoids computing the `phi` variational parameter directly using the\n",
      " |      optimization presented in **Lee, Seung: Algorithms for non-negative matrix factorization, NIPS 2001**.\n",
      " |  \n",
      " |  init_dir_prior(self, prior, name)\n",
      " |  \n",
      " |  log_perplexity(self, chunk, total_docs=None)\n",
      " |      Calculate and return per-word likelihood bound, using the `chunk` of\n",
      " |      documents as evaluation corpus. Also output the calculated statistics. incl.\n",
      " |      perplexity=2^(-bound), to log at INFO level.\n",
      " |  \n",
      " |  save(self, fname, ignore=['state', 'dispatcher'], separately=None, *args, **kwargs)\n",
      " |      Save the model to file.\n",
      " |      \n",
      " |      Large internal arrays may be stored into separate files, with `fname` as prefix.\n",
      " |      \n",
      " |      `separately` can be used to define which arrays should be stored in separate files.\n",
      " |      \n",
      " |      `ignore` parameter can be used to define which variables should be ignored, i.e. left\n",
      " |      out from the pickled lda model. By default the internal `state` is ignored as it uses\n",
      " |      its own serialisation not the one provided by `LdaModel`. The `state` and `dispatcher`\n",
      " |      will be added to any ignore parameter defined.\n",
      " |      \n",
      " |      \n",
      " |      Note: do not save as a compressed file if you intend to load the file back with `mmap`.\n",
      " |      \n",
      " |      Note: If you intend to use models across Python 2/3 versions there are a few things to\n",
      " |      keep in mind:\n",
      " |      \n",
      " |        1. The pickled Python dictionaries will not work across Python versions\n",
      " |        2. The `save` method does not automatically save all np arrays using np, only\n",
      " |           those ones that exceed `sep_limit` set in `gensim.utils.SaveLoad.save`. The main\n",
      " |           concern here is the `alpha` array if for instance using `alpha='auto'`.\n",
      " |      \n",
      " |      Please refer to the wiki recipes section (https://github.com/piskvorky/gensim/wiki/Recipes-&-FAQ#q9-how-do-i-load-a-model-in-python-3-that-was-trained-and-saved-using-python-2)\n",
      " |      for an example on how to work around these issues.\n",
      " |  \n",
      " |  show_topic(self, topicid, topn=10)\n",
      " |      Return a list of `(word, probability)` 2-tuples for the most probable\n",
      " |      words in topic `topicid`.\n",
      " |      \n",
      " |      Only return 2-tuples for the topn most probable words (ignore the rest).\n",
      " |  \n",
      " |  show_topics(self, num_topics=10, num_words=10, log=False, formatted=True)\n",
      " |      For `num_topics` number of topics, return `num_words` most significant words\n",
      " |      (10 words per topic, by default).\n",
      " |      \n",
      " |      The topics are returned as a list -- a list of strings if `formatted` is\n",
      " |      True, or a list of `(word, probability)` 2-tuples if False.\n",
      " |      \n",
      " |      If `log` is True, also output this result to log.\n",
      " |      \n",
      " |      Unlike LSA, there is no natural ordering between the topics in LDA.\n",
      " |      The returned `num_topics <= self.num_topics` subset of all topics is therefore\n",
      " |      arbitrary and may change between two LDA training runs.\n",
      " |  \n",
      " |  sync_state(self)\n",
      " |  \n",
      " |  top_topics(self, corpus, num_words=20)\n",
      " |      Calculate the Umass topic coherence for each topic. Algorithm from\n",
      " |      **Mimno, Wallach, Talley, Leenders, McCallum: Optimizing Semantic Coherence in Topic Models, CEMNLP 2011.**\n",
      " |  \n",
      " |  update(self, corpus, chunksize=None, decay=None, offset=None, passes=None, update_every=None, eval_every=None, iterations=None, gamma_threshold=None, chunks_as_numpy=False)\n",
      " |      Train the model with new documents, by EM-iterating over `corpus` until\n",
      " |      the topics converge (or until the maximum number of allowed iterations\n",
      " |      is reached). `corpus` must be an iterable (repeatable stream of documents),\n",
      " |      \n",
      " |      In distributed mode, the E step is distributed over a cluster of machines.\n",
      " |      \n",
      " |      This update also supports updating an already trained model (`self`)\n",
      " |      with new documents from `corpus`; the two models are then merged in\n",
      " |      proportion to the number of old vs. new documents. This feature is still\n",
      " |      experimental for non-stationary input streams.\n",
      " |      \n",
      " |      For stationary input (no topic drift in new documents), on the other hand,\n",
      " |      this equals the online update of Hoffman et al. and is guaranteed to\n",
      " |      converge for any `decay` in (0.5, 1.0>. Additionally, for smaller\n",
      " |      `corpus` sizes, an increasing `offset` may be beneficial (see\n",
      " |      Table 1 in Hoffman et al.)\n",
      " |      \n",
      " |      Args:\n",
      " |          corpus (gensim corpus): The corpus with which the LDA model should be updated.\n",
      " |      \n",
      " |          chunks_as_numpy (bool): Whether each chunk passed to `.inference` should be a np\n",
      " |              array of not. np can in some settings turn the term IDs\n",
      " |              into floats, these will be converted back into integers in\n",
      " |              inference, which incurs a performance hit. For distributed\n",
      " |              computing it may be desirable to keep the chunks as np\n",
      " |              arrays.\n",
      " |      \n",
      " |      For other parameter settings, see :class:`LdaModel` constructor.\n",
      " |  \n",
      " |  update_alpha(self, gammat, rho)\n",
      " |      Update parameters for the Dirichlet prior on the per-document\n",
      " |      topic weights `alpha` given the last `gammat`.\n",
      " |  \n",
      " |  update_eta(self, lambdat, rho)\n",
      " |      Update parameters for the Dirichlet prior on the per-topic\n",
      " |      word weights `eta` given the last `lambdat`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods defined here:\n",
      " |  \n",
      " |  load(cls, fname, *args, **kwargs) from __builtin__.type\n",
      " |      Load a previously saved object from file (also see `save`).\n",
      " |      \n",
      " |      Large arrays can be memmap'ed back as read-only (shared memory) by setting `mmap='r'`:\n",
      " |      \n",
      " |          >>> LdaModel.load(fname, mmap='r')\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from gensim.utils.SaveLoad:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from gensim.models.basemodel.BaseTopicModel:\n",
      " |  \n",
      " |  print_topic(self, topicno, topn=10)\n",
      " |      Return a single topic as a formatted string. See `show_topic()` for parameters.\n",
      " |      \n",
      " |      >>> lsimodel.print_topic(10, topn=5)\n",
      " |      '-0.340 * \"category\" + 0.298 * \"$M$\" + 0.183 * \"algebra\" + -0.174 * \"functor\" + -0.168 * \"operator\"'\n",
      " |  \n",
      " |  print_topics(self, num_topics=20, num_words=10)\n",
      " |      Alias for `show_topics()` that prints the `num_words` most\n",
      " |      probable words for `topics` number of topics to log.\n",
      " |      Set `topics=-1` to print all topics.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(Lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ldamodel = Lda(doc_term_matrix, num_topics=7, id2word = dictionary, passes = 100,alpha='auto',eval_every = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.10005151,  0.12048697,  0.10523123,  0.12867727,  0.18636753,\n",
       "        0.13314549,  0.40479572])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ldamodel.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  u'0.011*\"yordiprimeday\" + 0.011*\"exafm\" + 0.007*\"worldemojiday\" + 0.006*\"sandal\" + 0.006*\"tuesdaythoughts\" + 0.005*\"fatkiddeals\" + 0.005*\"latest\" + 0.005*\"amazonprimedaysale\" + 0.005*\"docker\" + 0.005*\"nintendo\" + 0.004*\"coupon\" + 0.004*\"tech\" + 0.004*\"gift\" + 0.004*\"switch\" + 0.004*\"und\" + 0.004*\"amazonecho\" + 0.004*\"mit\" + 0.003*\"nicht\" + 0.003*\"amazondeals\" + 0.003*\"etsy\"'),\n",
       " (1,\n",
       "  u'0.006*\"via\" + 0.006*\"baby\" + 0.005*\"plus\" + 0.005*\"gaming\" + 0.004*\"amazon\" + 0.004*\"instant\" + 0.004*\"primedaydeals\" + 0.004*\"pot\" + 0.004*\"pro\" + 0.004*\"water\" + 0.004*\"foundation\" + 0.004*\"fortnite\" + 0.003*\"le\" + 0.003*\"exclusive\" + 0.003*\"complete\" + 0.003*\"top\" + 0.003*\"raw\" + 0.003*\"youshopamazongives\" + 0.003*\"collection\" + 0.003*\"prime\"'),\n",
       " (2,\n",
       "  u'0.019*\"jaybird\" + 0.016*\"worker\" + 0.011*\"strike\" + 0.010*\"bezos\" + 0.008*\"food\" + 0.008*\"low\" + 0.007*\"condition\" + 0.007*\"man\" + 0.007*\"pay\" + 0.006*\"eye\" + 0.006*\"richest\" + 0.006*\"modern\" + 0.005*\"history\" + 0.005*\"mark\" + 0.005*\"becomes\" + 0.005*\"brutal\" + 0.005*\"di\" + 0.004*\"cat\" + 0.004*\"philip\" + 0.004*\"toothbrush\"'),\n",
       " (3,\n",
       "  u'0.008*\"ebay\" + 0.008*\"smart\" + 0.006*\"from\" + 0.006*\"book\" + 0.005*\"headphone\" + 0.005*\"pick\" + 0.005*\"review\" + 0.005*\"best\" + 0.004*\"free\" + 0.004*\"account\" + 0.004*\"home\" + 0.004*\"alexa\" + 0.004*\"event\" + 0.004*\"vacuum\" + 0.004*\"amazoncom\" + 0.004*\"shop\" + 0.004*\"live\" + 0.004*\"kit\" + 0.004*\"come\" + 0.004*\"guide\"'),\n",
       " (4,\n",
       "  u'0.011*\"sale\" + 0.008*\"via\" + 0.008*\"amazon\" + 0.007*\"love\" + 0.006*\"it\" + 0.006*\"new\" + 0.006*\"coffee\" + 0.005*\"im\" + 0.005*\"here\" + 0.005*\"set\" + 0.005*\"free\" + 0.005*\"rt\" + 0.005*\"now\" + 0.005*\"price\" + 0.004*\"checkout\" + 0.004*\"check\" + 0.004*\"bag\" + 0.004*\"book\" + 0.004*\"one\" + 0.004*\"today\"'),\n",
       " (5,\n",
       "  u'0.041*\"de\" + 0.019*\"el\" + 0.019*\"la\" + 0.017*\"en\" + 0.015*\"yordienexa\" + 0.014*\"que\" + 0.011*\"para\" + 0.009*\"con\" + 0.008*\"por\" + 0.008*\"un\" + 0.007*\"tv\" + 0.007*\"mi\" + 0.006*\"fire\" + 0.006*\"yordirosado\" + 0.006*\"stick\" + 0.006*\"yordiprimeday\" + 0.005*\"del\" + 0.005*\"los\" + 0.005*\"hoy\" + 0.005*\"da\"'),\n",
       " (6,\n",
       "  u'0.051*\"deal\" + 0.010*\"best\" + 0.009*\"day\" + 0.009*\"time\" + 0.008*\"amazonsmile\" + 0.008*\"price\" + 0.007*\"one\" + 0.007*\"great\" + 0.007*\"like\" + 0.007*\"go\" + 0.007*\"u\" + 0.006*\"check\" + 0.006*\"it\" + 0.006*\"good\" + 0.006*\"need\" + 0.006*\"year\" + 0.005*\"c\" + 0.005*\"support\" + 0.005*\"off\" + 0.005*\"today\"')]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ldamodel.print_topics(num_topics=7, num_words=20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
